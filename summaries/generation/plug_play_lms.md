This paper deals the problem of controllable text generation from language models. They adopt the approach used by Nguyen et.al 2017 paper which used an attribute models for image generation. The main idea is to combine pretrained transformer language models with simple attribute classifiers. They experiment with two types of attribute classifiers: BoW models and a simple MLP model. The approach is modular is the sense that multiple attribute models can be combined to produce text complaint with all the attributes involved.  The problem is modelled based on the following probability expression: p(x|a) ~ p(x)p(a|x). The p(x) component comes from the language model and p(a|x) component comes from the attribute models. The thing I like about the approach is that the architecture is fully modular, can be used with any language model and the attribute model can be added after training of LM.  No pretraining/fine-tuning of LM is needed to add a new attribute. Moreover, these attribute models are small as compared to the base LM so training the attribute models is not much of an overhead. 

The generation process involves three steps. In the first step, the forward pass across the transformer is made to get the p(x) and hidden states. The second step involves making a backward pass on the hidden states of the transformer based on maximizing the p(a|x) from the attribute model. One the hidden state has been recomputed, the sampling in done based on top-k.  There are a couple of tunable parameters, alpha: the learning rate equivalent, m: number of times the update should be applied, gamma: the scaling coefficient.
The approach makes a tradeoff between balancing the fluency and latent updates by combining the probability distribution in a geometric mean weighted by a parameter gamma. KL divergence is also used to make step towards ensuring fluency. 

The paper does extensive evaluation by considering the strong baseline using different techniques. Also, human eval was done extensively to give qualitative and quantitative results apart from automated metrics.  I found the paper a bit difficult to read, especially a lot of previous work is unknown to me, and they left a lot of details to appendix without which it was very difficult to understand the paper.

Questions/Concerns:

1.	Where and how did they use Metropolis adjusted Langevin Algorithm for sampling? They mentioned using top-k sampling procedure in results section.
2.	The challenge is setting up alpha and gamma parameters for the task. Though small values work well (as shown in results), but the process has been brittle to these parameters and it may not work on all attributes equally well.
3.	I have concerns about the mode collapse problem with Bow Attribute models. Since the collection is a set of fixed words, the model might just condition LM to generate sentences containing a few words leading very generic sentences with little diversity. This is also conveyed by the evaluation results.
4.	I did not quite understand eq. $ in the paper for the BoW Model. Why addition is done instead to maybe doing a product?
5.	I am not sure how this method will perform on fine grained topics, like maybe a species of a plant, or a specific field of physics maybe optics. The LM training data will have data from newspaper/internet etc. which have a majority of articles about politics, military etc. on which they evaluated.
Future Work:
1.	One good avenue to explore will be coupling knowledge graphs and producing text based on soft constraints based on relations in the KG. 
