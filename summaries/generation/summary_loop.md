The paper handles the problem of unsupervised summarization. 

Contributions: - 

1.    Introduction of a coverage model that measures the number of important aspects covered in the generated summary. 

2.    The training mechanism Summary Loop, leveraging coverage and fluency signals to guide the summarization model. 

3.    Use of heuristics while training to avoid some common errors like reducing repetition, incomplete sentences, and template filling. 

The base architecture extends the GPT-2 model by adding an unsupervised training objective and using RL to optimize the summary model. The summarizer model takes in document and the length parameter and generate a summary. This generated summary is passed to coverage and fluency models which evaluate the summary and produce scores which are combined into a single score by weighted addition.  

For the coverage model, BERT is used, and tokens are masked in the document based on tf-idf weights. The coverage score is the difference in the precision of the tokens generated by giving summaries as input and the precision by not giving the summary an input.  The coverage model is trained using first-n words of the documents as the summary. Maybe experiments using random sentences as summary could have helped. First couple of sentences may miss a major chunk of information.  The fluency model is also a GPT-2 model with the sentence likelihood being used as a proxy for the quality of the sentence.  The paper uses a SCST method to train the setup. They compare the summary generated by greedy sampling and pure sampling and define the reward as a comparison of the pure sampled to greedy sampled.  Three “guard rails” are used to augment the reward function to avoid basic errors.  

Some Concerns: 

1.    Fluency model training procedure not described. Plus using model likelihood scores not, a good proxy for fluent text as discussed in the text degeneration paper.  

2.    The tf-idf approach to generate blanks is sound but would have preferred an ablation using random masking, language model lowest probability words being masked etc. for the benefit the specific masking procedure Is providing.  

3.    The RL value function uses greedy and pure sampled for comparison. Maybe some pretrained models could be used to generate summaries and compare the performance. I could not understand the motivation of using greedy and pure sampled sentences. 

The results of the summaries generated are pretty good. The method achieves the performance in the league of supervised methods establishes a strong baseline for unsupervised methods. Also, the modular architecture can facilitate inclusion of different metrics like accuracy detector, diversity score etc.  Plus, the method shows that the weights are good for initialization and used with a small set of ground truth summaries.  