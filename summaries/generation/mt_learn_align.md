The paper deals with machine translation using neural networks. The main contribution of the paper is an introduction to alignment mechanism to address the bottleneck of the fixed length encoding vector. The MT systems at the time of the paper suffered from poor performance in large sentences and the papers conjectures that it is due to a single fixed length vector being used to store the source sentence.  The alignment module allows to lookup relevant parts of source sentence while generating the target word. The system achieves close to SOTA results on English to French translation as compared to phrase-based methods.  
Bidirectional RNNS are used to encode the source sentence. A soft attention mechanism is used with the scoring function as a feedforward network. The vectors from the forward and backward pass of the encoder (a bidirectional RNN) are concatenated and the decoder calculates the content vector based on the attention scores and the vectors in the encoder.  
The results of the paper show improvement in translation performance for large sentences especially in 30-50-word range. Plus, the authors did some qualitative analysis of the attention values and found patterns which seem reasonable, like mostly diagonal matrix with a certain aberrations for reversed word order for French and English. One drawback of the approach is the extra computation introduced at each decoder step taking the complexity from O(n) to O(n^2).  
The paper is well written, and the work is interesting as it addresses a limitation in translation systems for long sentences. The qualitative analysis supports the conjecture of the authors and establishes the causal relation of the improvement to the alignment model. The paper is written clearly explaining most of the decisions and justifying the assumptions made. One thing I find arbitrary is the concatenation of backward and forward embeddings. Maybe some experiments trying with techniques like addition/max/matrix dot product, could have been tried. Other thing is that the experiments have been conducted on French which is SVO like English. Maybe experiments with SOV languages like Hindi or VSO languages would help to further verify the efficacy of alignment module. Future work can be to experiment with difference languages and maybe check compositionality of languages like French->English, English->Hindi with wrt attention matrices, to check for whether is learns some patterns about the language structure or is it just surface level learning based on the words.  
 
