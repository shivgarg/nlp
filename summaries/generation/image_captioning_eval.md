Task: Image Captioning.
Both human and automated metrics are used to evaluate captioning systems. The basic human captioning evaluation metric is based on common-sense evaluation giving ratings on a 1-5 Likert scale. The automated metrics used by most of the benchmarks are CIDEr, ROUGE-L and SPICE metrics.  All the metrics are directly proportional to the quality of the system, meaning higher values means a better system. 

ROUGE-L:  It captures how many of the ngrams occurring in the reference occur in the generated text too. Here, tokens are not required to be consecutive. Effectively, it tries to quantify the closely the word order is followed in the generated text when compared to reference text. A drawback of this metric is that it weighs all ngrams equally, i.e. some generic phrases are treated equally with phrases specific to the image, leading to divergence with human eval. 

CIDEr: It was introduced specifically for image captioning systems evaluation and it had high agreement with human evaluation. It compares the ngram present in the references sentences and the generated sentences. It weighs the ngrams based on the frequency of the ngram, by employing the tf-idf based weighing metrics.   The Cider score for N-grams is a cosine distance between tf-idf scores of ngrams generated from references and generated sentences. A weighted sum over up to 4-gram cider is the final score. The metric tries to find how many of the ngrams overlap, and it does them by weighing up the rare ngrams indicating importance of that ngram for the caption. This is an improvement from ROUGE and BLEU scores which tend to weigh each ngram equally.  

SPICE: This metric tries to evaluate generated captions semantically with reference sentences by constructing scene graphs. The semantic relations are extracted from the graph for both reference and generated and a F1-score is calculated between the generated tuple set and the reference tuple set.  This metric is different from above two, as in it does not base the eval on statistics of common words. The sentence maybe a bag of words with poor grammar but still have a high similarity semantically with reference sentences.

I believe one promising metric to try using with captioning is HUSE. It measures diversity and quality for the generated sentence. The eval metrics used currently lack evaluating for diversity. Also, all the above-mentioned metrics do not evaluate any out of context generation. Like if the model generates a long caption with phrases not related to the image, ROUGE-L and CIDEr will not penalize hallucinated text.  Maybe BLEU can help in this case to penalize hallucinated phenomena. 
